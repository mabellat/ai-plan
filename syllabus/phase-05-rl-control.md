# Phase 5 — RL & Control (Weeks 33–36)

## RL Essentials
- MDPs: states, actions, rewards; Bellman equations.
- Value‑based: Monte‑Carlo, TD, Q‑learning, DQN.
- Policy‑based: REINFORCE; actor–critic; PPO.
- Exploration, entropy regularization; reward shaping; advantage estimation (GAE).

## RL for LLMs
- RLHF pipeline overview; synthetic feedback; offline RL vs on‑policy; DPO/KTO comparisons.

## Deliverables
- Implement PPO for a toy env; then apply to a text‑bandit preference task.
- Report: tradeoffs of RLHF vs DPO for your use‑case.
