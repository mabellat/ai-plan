{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a756a8c5",
   "metadata": {},
   "source": [
    "# Week 26 — Policy Gradients & Variance Reduction\n",
    "\n",
    "*Last updated:* 2025-09-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ade0c",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- [ ] Understand policy gradients & variance reduction\n",
    "- [ ] Complete guided exercises (theory → code → evaluation)\n",
    "- [ ] Apply learning in a small project or lab\n",
    "- [ ] Reflect using self-assessment checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fd554",
   "metadata": {},
   "source": [
    "## Mini-Theory (Deep Dive)\n",
    "- REINFORCE; baselines; entropy regularization\n",
    "- Advantage estimation (GAE) intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef2b0e9",
   "metadata": {},
   "source": [
    "## Guided Exercises\n",
    "    The following exercises are structured to help you learn by doing. Each has **starter code**, **hints**, and **checks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba59fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Implement tabular Q-learning on a simple gridworld\n",
    "import numpy as np, random\n",
    "\n",
    "n_states, n_actions = 16, 4\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "alpha, gamma, eps = 0.1, 0.95, 0.1\n",
    "\n",
    "def step(s, a):\n",
    "    s2 = (s + (1 if a==0 else -1)) % n_states\n",
    "    r = 1.0 if s2 == n_states-1 else -0.01\n",
    "    done = s2 == n_states-1\n",
    "    return s2, r, done\n",
    "\n",
    "for ep in range(300):\n",
    "    s = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(Q[s]) if random.random() > eps else random.randrange(n_actions)\n",
    "        s2, r, done = step(s, a)\n",
    "        Q[s, a] += alpha * (r + gamma * Q[s2].max() - Q[s, a])\n",
    "        s = s2\n",
    "print(\"Q[0]:\", Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba0394",
   "metadata": {},
   "source": [
    "## Project Work\n",
    "- This week connects to: `syllabus/phase-05-rl-control.md`\n",
    "- Implement the **Build** task described in the project README. Tie your notebook experiments into that code (e.g., import your module or save artifacts for the project).\n",
    "\n",
    "### Deliverable\n",
    "- A short write-up (5–10 bullets) on **what worked, what didn’t, and what you’ll try next**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee90d26",
   "metadata": {},
   "source": [
    "## Self-Assessment Checklist\n",
    "- [ ] I can explain the key concepts of **Policy Gradients & Variance Reduction** in my own words.\n",
    "- [ ] I completed the guided exercises and validated outputs.\n",
    "- [ ] I produced a small artifact (code, plot, or report) and linked it to the project.\n",
    "- [ ] I captured 3–5 learnings and 2 next steps.\n",
    "\n",
    "---\n",
    "**Tip:** Keep each week to ~10 hours: ~3h study, ~3h coding, ~3h project, ~1h reflection."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
