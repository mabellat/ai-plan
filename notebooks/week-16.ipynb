{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba58ba73",
   "metadata": {},
   "source": [
    "# Week 16 — RNNs/GRUs/LSTMs & Sequence Modeling\n",
    "\n",
    "*Last updated:* 2025-09-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a773808",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- [ ] Understand rnns/grus/lstms & sequence modeling\n",
    "- [ ] Complete guided exercises (theory → code → evaluation)\n",
    "- [ ] Apply learning in a small project or lab\n",
    "- [ ] Reflect using self-assessment checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e722d",
   "metadata": {},
   "source": [
    "## Mini-Theory (Deep Dive)\n",
    "- Vanishing/exploding gradients; gating; teacher forcing\n",
    "- Character-level modeling toy task\n",
    "- When to still use RNNs vs Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ebf84",
   "metadata": {},
   "source": [
    "## Guided Exercises\n",
    "    The following exercises are structured to help you learn by doing. Each has **starter code**, **hints**, and **checks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Minimal PyTorch training loop w/ AMP & gradient clipping\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X = torch.randn(2048, 32)\n",
    "y = torch.randint(0, 4, (2048,))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=32, hidden=128, out_dim=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, out_dim))\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = MLP()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "ds = TensorDataset(X, y)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for xb, yb in dl:\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "    print(\"epoch\", epoch, \"loss\", float(loss))\n",
    "# TODO: add validation loop and accuracy metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d2b0b",
   "metadata": {},
   "source": [
    "## Project Work\n",
    "- This week connects to: `code/dl/train_loop.py`\n",
    "- Implement the **Build** task described in the project README. Tie your notebook experiments into that code (e.g., import your module or save artifacts for the project).\n",
    "\n",
    "### Deliverable\n",
    "- A short write-up (5–10 bullets) on **what worked, what didn’t, and what you’ll try next**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421d154",
   "metadata": {},
   "source": [
    "## Self-Assessment Checklist\n",
    "- [ ] I can explain the key concepts of **RNNs/GRUs/LSTMs & Sequence Modeling** in my own words.\n",
    "- [ ] I completed the guided exercises and validated outputs.\n",
    "- [ ] I produced a small artifact (code, plot, or report) and linked it to the project.\n",
    "- [ ] I captured 3–5 learnings and 2 next steps.\n",
    "\n",
    "---\n",
    "**Tip:** Keep each week to ~10 hours: ~3h study, ~3h coding, ~3h project, ~1h reflection."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
