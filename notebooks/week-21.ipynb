{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e42ce1",
   "metadata": {},
   "source": [
    "# Week 21 — Parameter-Efficient Finetuning (LoRA/QLoRA/Adapters)\n",
    "\n",
    "*Last updated:* 2025-09-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad30cce",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- [ ] Understand parameter-efficient finetuning (lora/qlora/adapters)\n",
    "- [ ] Complete guided exercises (theory → code → evaluation)\n",
    "- [ ] Apply learning in a small project or lab\n",
    "- [ ] Reflect using self-assessment checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f2057",
   "metadata": {},
   "source": [
    "## Mini-Theory (Deep Dive)\n",
    "- PEFT landscape; low-rank adaptation; quantization-aware finetuning\n",
    "- Memory/throughput measurements; ablation design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335b91e",
   "metadata": {},
   "source": [
    "## Guided Exercises\n",
    "    The following exercises are structured to help you learn by doing. Each has **starter code**, **hints**, and **checks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Tokenization or PEFT playground (choose)\n",
    "import torch\n",
    "def lora_update(W, A, B, alpha=1.0):\n",
    "    return W + alpha * (A @ B)\n",
    "W = torch.randn(64, 64); A = torch.randn(64, 4); B = torch.randn(4, 64)\n",
    "W_hat = lora_update(W, A, B, alpha=0.1)\n",
    "print(\"Delta norm:\", (W_hat - W).norm().item())\n",
    "# TODO: integrate into a tiny nn.Linear forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4089772",
   "metadata": {},
   "source": [
    "## Project Work\n",
    "- This week connects to: `projects/21-qlora-finetune/README.md`\n",
    "- Implement the **Build** task described in the project README. Tie your notebook experiments into that code (e.g., import your module or save artifacts for the project).\n",
    "\n",
    "### Deliverable\n",
    "- A short write-up (5–10 bullets) on **what worked, what didn’t, and what you’ll try next**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c5069",
   "metadata": {},
   "source": [
    "## Self-Assessment Checklist\n",
    "- [ ] I can explain the key concepts of **Parameter-Efficient Finetuning (LoRA/QLoRA/Adapters)** in my own words.\n",
    "- [ ] I completed the guided exercises and validated outputs.\n",
    "- [ ] I produced a small artifact (code, plot, or report) and linked it to the project.\n",
    "- [ ] I captured 3–5 learnings and 2 next steps.\n",
    "\n",
    "---\n",
    "**Tip:** Keep each week to ~10 hours: ~3h study, ~3h coding, ~3h project, ~1h reflection."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
